<h1>Caching Feature Specification</h1>
<p>The caching feature enables the storage and retrieval of frequently accessed data to enhance application performance by reducing redundant computations or database queries. It provides a configurable, extensible caching system that integrates seamlessly with services, supporting multiple storage backends and offering robust mechanisms for cache management.</p>
<h2>Table of Contents</h2>
<ol>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#cache-storage">Cache Storage</a></li>
<li><a href="#configuration">Configuration</a></li>
<li><a href="#cache-operations">Cache Operations</a></li>
<li><a href="#cache-key-generation">Cache Key Generation</a></li>
<li><a href="#cache-invalidation">Cache Invalidation</a></li>
<li><a href="#integration-with-services">Integration with Services</a><ul>
<li><a href="#declarative-caching">Declarative Caching</a></li>
<li><a href="#transparent-operation">Transparent Operation</a></li>
<li><a href="#automatic-cache-management-actions">Automatic Cache Management Actions</a></li>
</ul>
</li>
<li><a href="#error-handling">Error Handling</a></li>
<li><a href="#performance-considerations">Performance Considerations</a></li>
<li><a href="#security">Security</a></li>
<li><a href="#monitoring-and-logging">Monitoring and Logging</a></li>
<li><a href="#implementation-notes">Implementation Notes</a></li>
<li><a href="#examples">Examples</a><ul>
<li><a href="#configuration-example">Configuration Example</a></li>
<li><a href="#service-definition-example">Service Definition Example</a></li>
<li><a href="#cache-invalidation-example">Cache Invalidation Example</a></li>
</ul>
</li>
</ol>
<h2>Introduction</h2>
<p>The caching feature enables the storage and retrieval of frequently accessed data to enhance application performance by reducing redundant computations or database queries. It provides a configurable, extensible caching system that integrates seamlessly with services, supporting multiple storage backends and offering robust mechanisms for cache management.</p>
<h2>Cache Storage</h2>
<p><strong>Description</strong>: The system supports multiple cache storage backends to suit different scalability and performance needs.</p>
<p><strong>Supported Backends</strong>:</p>
<ul>
<li><strong>In-memory Cache</strong>: A lightweight, fast cache stored in the application&#39;s memory (e.g., using a hash map)</li>
<li><strong>DHT Cache</strong>: A distributed cache using the P2P network&#39;s DHT system</li>
<li><strong>Custom Backends</strong>: Users can implement and integrate custom cache stores, including third-party solutions like Redis if needed</li>
</ul>
<p><strong>Configuration</strong>: The backend is selectable via configuration files (e.g., JSON/YAML) or environment variables (e.g., <code>CACHE_BACKEND=dht</code>).</p>
<h3>DHT Cache Backend</h3>
<p>When using the DHT as a cache backend, the system:</p>
<ul>
<li>Stores cache entries in the current network&#39;s DHT (network ID retrieved from context)</li>
<li>Uses TTL-based expiration through DHT value expiration</li>
<li>Provides automatic replication across peers</li>
<li>Supports network-specific cache isolation</li>
</ul>
<p><strong>DHT Cache Configuration</strong>:</p>
<pre><code class="language-yaml">cache:
  backend: dht
  replication: 3     # Number of replicas
  ttl: 3600          # Default TTL in seconds
  prefix: &quot;cache:&quot;   # Key prefix in DHT
</code></pre>
<h2>Configuration</h2>
<h3>Global Configuration:</h3>
<ul>
<li>Enable or disable caching system-wide</li>
<li>Set a default TTL (Time To Live) for cache entries (e.g., 30 seconds)</li>
</ul>
<h3>Service-Level Configuration:</h3>
<ul>
<li>Override global settings for specific services</li>
<li>Example: Enable caching only for a subset of services</li>
</ul>
<h3>Action-Level Configuration:</h3>
<ul>
<li>Enable/disable caching per action</li>
<li>Specify custom TTL or caching conditions per action</li>
<li>Example: <code>#[action(cache(enabled = true, ttl = 60))]</code></li>
</ul>
<h2>Cache Operations</h2>
<p>The following diagram illustrates the cache operations flow:</p>
<pre><code class="language-mermaid">flowchart TD
    A[Request Received] --&gt; B{Cache Enabled?}
    B --&gt;|No| C[Execute Action]
    B --&gt;|Yes| D[Generate Cache Key]
    D --&gt; E{Key in Cache?}
    E --&gt;|Yes| F{Entry Expired?}
    E --&gt;|No| G[Execute Action]
    F --&gt;|Yes| G
    F --&gt;|No| H[Return Cached Value]
    G --&gt; I[Store Result in Cache]
    I --&gt; J[Return Result]
    H --&gt; J
    
    K[Cache Invalidation] --&gt; L{Invalidation Type}
    L --&gt;|Time-based| M[Auto-expire after TTL]
    L --&gt;|Action-based| N[Explicit Cache Clear]
    L --&gt;|Event-based| O[Event Triggers Invalidation]
</code></pre>
<h3>System-Level API:</h3>
<ul>
<li><code>set(key, value, ttl)</code>: Stores a value in the cache with a specified key and optional TTL (in seconds)</li>
<li><code>get(key)</code>: Retrieves the value associated with the key, returning null if not found or expired</li>
<li><code>delete(key)</code>: Removes a specific cache entry</li>
<li><code>clear()</code>: Deletes all entries in the cache</li>
</ul>
<p><strong>Note</strong>: These methods are available internally to the caching system but are not intended for direct use by service implementations. Services should use the auto-generated cache management actions.</p>
<h2>Cache Key Generation</h2>
<p><strong>Automatic Generation</strong>: Unique keys are generated based on the action name and its parameters (e.g., <code>getUser:123</code> for action <code>getUser</code> with parameter <code>id=123</code>)</p>
<p><strong>Custom Keys</strong>: Users can define custom keys or provide a key generation function (e.g., <code>key: (params) =&gt; &quot;custom:&quot; + params.id</code>)</p>
<h2>Cache Invalidation</h2>
<p><strong>Time-based</strong>: Entries expire automatically after their TTL</p>
<p><strong>Action-based Invalidation</strong>: Cache entries are invalidated using the auto-generated cache management actions:</p>
<ul>
<li><code>&lt;service&gt;/cache/clear</code>: Clear all cache entries for the service</li>
<li><code>&lt;service&gt;/cache/delete</code>: Delete a specific cache entry by key</li>
<li><code>&lt;service&gt;/cache/revoke</code>: Revoke cache entries by pattern matching</li>
</ul>
<p><strong>Event-based</strong>: Invalidate cache entries when specific events occur (e.g., a <code>user.updated</code> event clears related cache keys)</p>
<ul>
<li>Configurable via event mappings (e.g., <code>{ event: &quot;user.updated&quot;, keys: [&quot;getUser:*&quot;] }</code>)</li>
</ul>
<h2>Integration with Services</h2>
<h3>Declarative Caching</h3>
<p>Enable caching for actions using macros:</p>
<ul>
<li>Example: <code>#[cached(ttl = 60)]</code> or <code>#[action(cache(enabled = true, ttl = 60))]</code> in service definitions</li>
</ul>
<blockquote>
<p><strong>Note</strong>: Kagi macros support both compile-time (distributed slices) and runtime registration approaches, making them fully compatible with testing environments without requiring unstable Rust features.</p>
</blockquote>
<h3>Transparent Operation</h3>
<p>Caching operates transparently to service implementations:</p>
<ol>
<li>When a request is received, the caching system checks if a valid cache entry exists</li>
<li>If a cache hit occurs, the cached result is returned immediately without invoking the action handler</li>
<li>If a cache miss occurs, the action handler is invoked and its result is stored in the cache</li>
</ol>
<p>This means service implementations don&#39;t need to handle caching logic directly - the system manages it automatically.</p>
<h3>Automatic Cache Management Actions</h3>
<p>When caching is enabled for a service, the system automatically adds the following cache management actions:</p>
<ul>
<li><code>&lt;service&gt;/cache/clear</code>: Clears all cache entries for the service</li>
<li><code>&lt;service&gt;/cache/delete</code>: Deletes a specific cache entry by key</li>
<li><code>&lt;service&gt;/cache/revoke</code>: Revokes cache entries by pattern matching</li>
</ul>
<p>Example for a <code>user</code> service:</p>
<pre><code>user/cache/clear                                  -&gt; Clears all user service cache entries
user/cache/delete {key: &quot;get_user:123&quot;}          -&gt; Deletes specific cache entry
user/cache/revoke {pattern: &quot;get_user:*&quot;}        -&gt; Invalidates all cache entries matching the pattern
</code></pre>
<p>These automatically generated endpoints provide a complete interface for cache management and can be used by services, admin interfaces, or for troubleshooting.</p>
<h3>P2P Cache Integration</h3>
<p>When using the DHT cache backend, the system integrates with the P2P transport layer:</p>
<ol>
<li><p><strong>DHT Storage</strong>:</p>
<pre><code class="language-rust">impl DHTCacheBackend {
    async fn set(&amp;self, key: String, value: Vec&lt;u8&gt;, ttl: Duration) -&gt; Result&lt;()&gt; {
        // Get network_id from context
        let network_id = self.context.network_id();
        
        // Store in current network&#39;s DHT
        self.p2p.dht_put(
            network_id,
            format!(&quot;{}:{}&quot;, self.prefix, key),
            CacheEntry {
                value,
                expires_at: SystemTime::now() + ttl,
            }.serialize()
        ).await
    }
    
    async fn get(&amp;self, key: String) -&gt; Result&lt;Option&lt;Vec&lt;u8&gt;&gt;&gt; {
        // Get network_id from context
        let network_id = self.context.network_id();
        
        // Retrieve from current network&#39;s DHT
        if let Some(data) = self.p2p.dht_get(
            network_id,
            format!(&quot;{}:{}&quot;, self.prefix, key)
        ).await? {
            let entry: CacheEntry = deserialize(&amp;data)?;
            if entry.expires_at &gt; SystemTime::now() {
                return Ok(Some(entry.value));
            }
        }
        Ok(None)
    }
}
</code></pre>
</li>
<li><p><strong>Network Events</strong>:</p>
<ul>
<li>Subscribe to network events for cache invalidation</li>
<li>Handle peer join/leave events for replication</li>
<li>Coordinate cache updates across peers</li>
</ul>
</li>
<li><p><strong>Replication Strategy</strong>:</p>
<pre><code class="language-rust">impl DHTCacheBackend {
    async fn ensure_replication(&amp;self, key: String) -&gt; Result&lt;()&gt; {
        // Get network_id from context
        let network_id = self.context.network_id();
        
        let peers = self.p2p.get_network_peers(network_id).await?;
        if peers.len() &lt; self.config.replication {
            // Trigger replication to new peers
            self.replicate_cache_entry(key).await?;
        }
        Ok(())
    }
    
    async fn replicate_cache_entry(&amp;self, key: String) -&gt; Result&lt;()&gt; {
        if let Some(entry) = self.get(key.clone()).await? {
            // Get network_id from context
            let network_id = self.context.network_id();
            
            // Replicate to additional peers
            self.p2p.dht_put_replicated(
                network_id,
                format!(&quot;{}:{}&quot;, self.prefix, key),
                entry,
                self.config.replication
            ).await?;
        }
        Ok(())
    }
}
</code></pre>
</li>
<li><p><strong>Cache Consistency</strong>:</p>
<ul>
<li>Use DHT&#39;s eventual consistency model</li>
<li>Handle conflicts through timestamp-based resolution</li>
<li>Propagate invalidations across the network</li>
</ul>
</li>
</ol>
<h2>Error Handling</h2>
<p><strong>Fallback Mechanism</strong>: If the cache backend is unavailable or fails, the system bypasses the cache and fetches data directly</p>
<p><strong>Error Logging</strong>: Cache-related errors (e.g., connection failures) are logged for debugging, without disrupting service operation</p>
<h2>Performance Considerations</h2>
<p><strong>Low Overhead</strong>: Cache operations are optimized for speed, using asynchronous methods where applicable</p>
<p><strong>In-memory Priority</strong>: Recommend in-memory caching for low-latency scenarios, with custom backends for scalability</p>
<p><strong>Concurrency</strong>: Ensure thread-safe access for in-memory caches in multi-threaded environments</p>
<h2>Security</h2>
<p><strong>Data Sensitivity</strong>: Avoid caching sensitive data unless explicitly secured</p>
<p><strong>External Backends</strong>: Use encrypted connections when caching data externally with custom backends</p>
<p><strong>Access Control</strong>: Restrict cache access to authorized services or processes</p>
<h2>Monitoring and Logging</h2>
<p><strong>Metrics</strong>: Provide optional metrics for cache performance (e.g., hit rate, miss rate, operation latency)</p>
<p><strong>Logging</strong>: Log cache events (e.g., hits, misses, errors) for debugging and optimization</p>
<ul>
<li>Example: <code>Cache hit: getUser:123</code>, <code>Cache miss: getUser:456</code></li>
</ul>
<h2>Implementation Notes</h2>
<p><strong>Extensibility</strong>: The system allows plugging in custom cache backends via a defined interface (e.g., <code>{ get, set, delete, clear }</code>)</p>
<p><strong>Minimal Impact</strong>: Caching logic is decoupled from business logic, ensuring clean service code</p>
<p><strong>Scalability</strong>: Support for distributed caches ensures compatibility with multi-node deployments</p>
<h2>Examples</h2>
<h3>Configuration Example</h3>
<pre><code class="language-json">{
  &quot;cache&quot;: {
    &quot;enabled&quot;: true,
    &quot;backend&quot;: &quot;dht&quot;,
    &quot;ttl&quot;: 30,
    &quot;replication&quot;: 3,
    &quot;prefix&quot;: &quot;cache:&quot;
  },
  &quot;services&quot;: {
    &quot;user&quot;: {
      &quot;actions&quot;: {
        &quot;get_user&quot;: {
          &quot;cache&quot;: {
            &quot;enabled&quot;: true,
            &quot;ttl&quot;: 60,
            &quot;keys&quot;: [&quot;id&quot;]
          }
        }
      }
    }
  }
}
</code></pre>
<h3>Service Definition Example</h3>
<pre><code class="language-rust">use kagi_node::prelude::*;

// Define service with caching using macros
#[kagi::service]
struct UserService {
    #[inject]
    db_connection: DatabaseConnection,
}

impl UserService {
    // Cached action with custom TTL using the action macro with cache attributes
    #[action(cache(enabled = true, ttl = 60))]
    async fn get_user(&amp;self, context: &amp;RequestContext, id: u64) -&gt; Result&lt;User&gt; {
        // This will only be called on cache miss
        // The caching system has already checked for a cached result before calling this method
        let user = self.db_connection.query_one(&quot;SELECT * FROM users WHERE id = ?&quot;, &amp;[id]).await?;
        Ok(user)
    }
    
    // Alternative syntax with dedicated cached macro
    #[action]
    #[cached(ttl = 60)]
    async fn get_user_profile(&amp;self, context: &amp;RequestContext, id: u64) -&gt; Result&lt;UserProfile&gt; {
        let profile = self.db_connection.query_one(&quot;SELECT * FROM profiles WHERE user_id = ?&quot;, &amp;[id]).await?;
        Ok(profile)
    }
    
    // Action that invalidates cache
    #[action]
    async fn update_user(&amp;self, context: &amp;RequestContext, id: u64, data: UserData) -&gt; Result&lt;User&gt; {
        let user = self.db_connection.update(&quot;users&quot;, id, &amp;data).await?;
        
        // Invalidate the cache for this user using the cache management action
        let params = vmap! {
            &quot;key&quot; =&gt; format!(&quot;get_user:{}&quot;, id)
        };
        context.request(&quot;user/cache/delete&quot;, params).await?;
        
        // Publish event that will trigger cache invalidation rules
        context.publish(&quot;user.updated&quot;, json!({ &quot;id&quot;: id })).await?;
        
        Ok(user)
    }
}
</code></pre>
<h3>Cache Invalidation Example</h3>
<p>Here&#39;s how cache invalidation works in practice:</p>
<pre><code class="language-rust">// Example of handling an update that requires cache invalidation
#[action]
async fn update_profile(&amp;self, context: &amp;RequestContext, user_id: u64, profile_data: ProfileData) -&gt; Result&lt;Profile&gt; {
    // Update the profile in the database
    let updated_profile = self.db.update_profile(user_id, &amp;profile_data).await?;
    
    // Invalidate specific cache entry using the cache management action
    let params = vmap! {
        &quot;key&quot; =&gt; format!(&quot;get_profile:{}&quot;, user_id)
    };
    context.request(&quot;profile/cache/delete&quot;, params).await?;
    
    // For invalidating multiple related entries:
    let params = vmap! {
        &quot;pattern&quot; =&gt; format!(&quot;profile:user:{}:*&quot;, user_id)
    };
    context.request(&quot;profile/cache/revoke&quot;, params).await?;
    
    Ok(updated_profile)
}
</code></pre>
<p>This caching system design ensures:</p>
<ol>
<li>Separation of concerns - services don&#39;t need to manage cache directly</li>
<li>Performance - cached results are returned without invoking handlers</li>
<li>Consistency - cache invalidation is handled through standard service actions</li>
<li>Manageability - all cache operations are available as standard service actions</li>
<li>Flexibility - configurable at global, service, and action levels</li>
</ol>
<h3>DHT Cache Example</h3>
<pre><code class="language-yaml"># DHT cache configuration
cache:
  backend: dht
  replication: 3
  ttl: 3600
  prefix: &quot;cache:&quot;
  consistency:
    strategy: &quot;eventual&quot;
    conflict_resolution: &quot;last_write_wins&quot;
</code></pre>
<pre><code class="language-rust">// Service using DHT cache
#[service]
struct ProfileService {
    #[inject]
    db: Database,
}

impl ProfileService {
    // Using DHT cache with the cached macro
    #[action]
    #[cached(backend = &quot;dht&quot;, ttl = 60)]
    async fn get_user_profile(&amp;self, context: &amp;RequestContext, user_id: String) -&gt; Result&lt;Profile&gt; {
        // Function implementation
        // Network ID automatically retrieved from context
        // Caching handled automatically by the system
        let profile = self.db.get_profile(user_id).await?;
        Ok(profile)
    }

    // Manual cache management
    #[action]
    async fn update_profile(&amp;self, context: &amp;RequestContext, user_id: String, profile: Profile) -&gt; Result&lt;()&gt; {
        // Update profile
        self.db.update_profile(user_id.clone(), profile).await?;
        
        // Invalidate cache within the current network
        context.request(
            &quot;profile/cache/delete&quot;,
            vmap! {
                &quot;key&quot; =&gt; format!(&quot;get_user_profile:{}&quot;, user_id)
            }
        ).await?;
        
        Ok(())
    }
}
</code></pre>
